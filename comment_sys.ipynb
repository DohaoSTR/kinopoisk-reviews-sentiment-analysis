{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Получение данных из файлов.\n"
      ],
      "metadata": {
        "id": "07tiTQpHlzgg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IUuiquiEkvyN"
      },
      "outputs": [],
      "source": [
        "from pandas.io.formats.style_render import DataFrame\n",
        "import os.path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def get_text_data(path, label = None):\n",
        "    X = []\n",
        "    for review in os.listdir(path):\n",
        "        with open(os.path.join(path,review)) as f:\n",
        "            rev = f.read()\n",
        "            \n",
        "        X.append(rev)\n",
        "        \n",
        "    y = [label]*len(X)\n",
        "        \n",
        "    return X, y\n",
        "\n",
        "def to_data_frame():\n",
        "  good_review = \"good/\"\n",
        "  neutral_review = \"neutral/\"\n",
        "  bad_review = \"bad/\"\n",
        "\n",
        "  good_X, good_y = get_text_data(good_review, label = \"good\")\n",
        "  neutral_X, neutral_y = get_text_data(neutral_review, label = \"neutral\")\n",
        "  bad_X, bad_y = get_text_data(bad_review, label = \"bad\")\n",
        "\n",
        "  X = np.concatenate((good_X, neutral_X, bad_X))\n",
        "  y = np.concatenate((good_y, neutral_y, bad_y))\n",
        "  data = np.column_stack((X, y))\n",
        "\n",
        "  return pd.DataFrame(data, columns = [\"review\", \"label\"])\n",
        "\n",
        "def get_size(path):\n",
        "   maxSize = 0;\n",
        "   for review in os.listdir(path):\n",
        "     size = os.path.getsize(path + review)\n",
        "\n",
        "     if (size > maxSize):\n",
        "       maxSize = size\n",
        "\n",
        "     #print(review + \" - \" + str(os.path.getsize(path + review)) + \" бит\")\n",
        "\n",
        "   return maxSize\n",
        "      \n",
        "def get_sizes():\n",
        "  print(\"Максимальная размерность нейтрального комментария - \" + str(get_size(\"neutral/\")) + \" бит\")\n",
        "  print(\"Максимальная размерность хорошего комментария - \" + str(get_size(\"good/\")) + \" бит\")\n",
        "  print(\"Максимальная размерность плохого комментария - \" + str(get_size(\"bad/\")) + \" бит\")\n",
        "\n",
        "def save_dataset(dataset: DataFrame, path: str):\n",
        "  dataset.to_pickle(path + \".pkl\")\n",
        "\n",
        "def get_dataset(path: str) -> DataFrame:\n",
        "  return pd.read_pickle(path + \".pkl\")\n",
        "\n",
        "# сохранение набора данных\n",
        "#dataset = to_data_frame()\n",
        "#save_dataset(dataset, \"dataset\")\n",
        "\n",
        "# получение и вывод на экран\n",
        "#dataset = get_dataset(\"dataset\")\n",
        "#dataset.head()\n",
        "\n",
        "#get_sizes()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Предварительная обработка данных:"
      ],
      "metadata": {
        "id": "qXoDrNQCAdDV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "\n",
        "import pandas as pd\n",
        "from pandas.io.formats.style_render import DataFrame\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "nlkt_stopwords = set(stopwords.words('russian'))\n",
        "\n",
        "!pip install https://github.com/explosion/spacy-models/releases/download/ru_core_news_sm-3.1.0/ru_core_news_sm-3.1.0.tar.gz\n",
        "import spacy\n",
        "sp = spacy.load('ru_core_news_sm')\n",
        "spacy_stopwords = sp.Defaults.stop_words\n",
        "\n",
        "from nltk.stem import SnowballStemmer\n",
        "stemmer = SnowballStemmer(language=\"russian\")\n",
        "\n",
        "!pip install pymorphy2\n",
        "import pymorphy2\n",
        "morph_analyzer = pymorphy2.MorphAnalyzer()\n",
        "\n",
        "from operator import itemgetter\n",
        "\n",
        "def get_dataset(path: str) -> DataFrame:\n",
        "  return pd.read_pickle(path + \".pkl\")\n",
        "\n",
        "def save_dataset(dataset: DataFrame, path: str):\n",
        "  dataset.to_pickle(path + \".pkl\")\n",
        "\n",
        "def save_dataset_xlsx(dataset: DataFrame, path: str):\n",
        "  dataset.to_excel(path + \".xlsx\")\n",
        "\n",
        "# метод возвращающий слова комментариев в виде списка, без символов и цифр\n",
        "def get_words_without_symbols(review: str) -> List[str]:\n",
        "  text_words = word_tokenize(review)\n",
        "  review = [word.lower() for word in text_words if word.isalpha()]\n",
        "  return review\n",
        "\n",
        "def get_words_without_stop_words(review: List[str]) -> List[str]:\n",
        "  review = [word for word in review if word not in nlkt_stopwords]\n",
        "  review = [word for word in review if word not in spacy_stopwords]\n",
        "  return review\n",
        "\n",
        "def get_lemmed_words(review: List[str]) -> List[str]:\n",
        "  review = [morph_analyzer.parse(word)[0].normal_form for word in review]\n",
        "  return review\n",
        "\n",
        "def get_stemmed_words(review: List[str]) -> List[str]:\n",
        "  review = [stemmer.stem(word) for word in review]\n",
        "  return review\n",
        "\n",
        "def get_cleaned_words(review: str) -> List[str]:\n",
        "  review = get_words_without_symbols(review)\n",
        "  review = get_words_without_stop_words(review)\n",
        "  review = get_lemmed_words(review)\n",
        "  review = get_stemmed_words(review)\n",
        "  review = [word for word in review if len(word) > 3]\n",
        "  return review\n",
        "\n",
        "def get_cleaned_reviews(reviews):\n",
        "  cleaned_reviews = []\n",
        "  for review in reviews:\n",
        "    words = get_cleaned_words(review)\n",
        "    cleaned_reviews.append(words)\n",
        "  return cleaned_reviews\n",
        "\n",
        "#dataset = get_dataset(\"dataset\")\n",
        "#reviews = dataset[\"review\"]\n",
        "\n",
        "#cleaned_reviews = get_cleaned_reviews(reviews)\n",
        "#data = np.column_stack((cleaned_reviews, dataset[\"label\"]))\n",
        "\n",
        "#dataset = pd.DataFrame(data, columns = [\"review\", \"label\"])\n",
        "\n",
        "#save_dataset(dataset, \"cleaned_dataset\")\n",
        "#save_dataset_xlsx(dataset, \"cleaned_dataset\")"
      ],
      "metadata": {
        "id": "E32gaYFMAb5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Получение словаря выборки:"
      ],
      "metadata": {
        "id": "EFJGMg-ZEG9X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "\n",
        "import pandas as pd\n",
        "import pickle\n",
        "\n",
        "from pandas.io.formats.style_render import DataFrame\n",
        "\n",
        "def get_dataset(path: str) -> DataFrame:\n",
        "  return pd.read_pickle(path + \".pkl\")\n",
        "\n",
        "def save_dataset(dataset: DataFrame, path: str):\n",
        "  dataset.to_pickle(path + \".pkl\")\n",
        "\n",
        "dataset = get_dataset(\"cleaned_dataset\")\n",
        "reviews = dataset[\"review\"]\n",
        "\n",
        "# получаем словарь выборки\n",
        "def get_dictionary(reviews) -> List[str]:\n",
        "  dictionary = {}\n",
        "\n",
        "  for review in reviews:\n",
        "    for word in review:\n",
        "      if word in dictionary:\n",
        "        dictionary[word] += 1\n",
        "      else:\n",
        "        dictionary[word] = 1\n",
        "\n",
        "  dictionary = list(dictionary.items())\n",
        "\n",
        "  for element in set(dictionary):\n",
        "    if element[1] == 1:\n",
        "        dictionary.remove(element)\n",
        "\n",
        "  dictionary = sorted(set(dictionary), key=itemgetter(1), reverse = True)\n",
        "  return dictionary\n",
        "\n",
        "dictionary = get_dictionary(reviews)\n",
        "\n",
        "words = []\n",
        "counts = []\n",
        "for element in dictionary:\n",
        "  words.append(element[0])\n",
        "  counts.append(element[1])\n",
        "\n",
        "dictionary = set(dict(zip(words, counts)))\n",
        "\n",
        "file = open('dictionary.bin', 'wb')\n",
        "pickle.dump(dictionary, file)\n",
        "\n",
        "print(dictionary)"
      ],
      "metadata": {
        "id": "hpM9En2yB3lt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Векторизация данных:"
      ],
      "metadata": {
        "id": "dlSjHToLERbL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import pickle\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from pandas.io.formats.style_render import DataFrame\n",
        "\n",
        "def get_dataset(path: str) -> DataFrame:\n",
        "  return pd.read_pickle(path + \".pkl\")\n",
        "\n",
        "dataset = get_dataset(\"cleaned_dataset\")\n",
        "reviews = dataset[\"review\"]\n",
        "\n",
        "newComments = []\n",
        "for comment in reviews.tolist():\n",
        "  newComments.append(str(comment))\n",
        "\n",
        "tfidfconverter = TfidfVectorizer(max_features=5500, min_df=40, max_df=0.82)\n",
        "X = tfidfconverter.fit_transform(newComments).toarray()"
      ],
      "metadata": {
        "id": "ZHXvfSLYEP7S"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Случайный лес"
      ],
      "metadata": {
        "id": "PJSpcBxGDUe6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import pickle\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "from pandas.io.formats.style_render import DataFrame\n",
        "\n",
        "def get_dataset(path: str) -> DataFrame:\n",
        "  return pd.read_pickle(path + \".pkl\")\n",
        "\n",
        "dataset = get_dataset(\"cleaned_dataset\")\n",
        "reviews = dataset[\"review\"]\n",
        "\n",
        "reviews_string = []\n",
        "for comment in reviews.tolist():\n",
        "  reviews_string.append(str(comment))\n",
        "\n",
        "def classification(max_feautures, min_df, max_df, n_estimators):\n",
        "  vectorizer = TfidfVectorizer(max_features=max_feautures, min_df = min_df, max_df = max_df)\n",
        "  reviews_vectorized = vectorizer.fit_transform(reviews_string).toarray()\n",
        "\n",
        "  x_train, x_test, y_train, y_test = train_test_split(reviews_vectorized, dataset[\"label\"], test_size=0.3, shuffle = True)\n",
        "\n",
        "  classifier = RandomForestClassifier(n_estimators=n_estimators)\n",
        "\n",
        "  classifier.fit(x_train, y_train) \n",
        "  y_pred = classifier.predict(x_test)\n",
        "\n",
        "  print(\"Отчёт по классификации: \")\n",
        "  print(classification_report(y_test, y_pred))\n",
        "  print(\"Точность: \" + str(accuracy_score(y_test, y_pred)))\n",
        "\n",
        "  return accuracy_score(y_test, y_pred);\n",
        "\n",
        "def test():\n",
        "  accuracy_list = []\n",
        "  value_list = []\n",
        "\n",
        "  for value in range(100, 1000, 50):\n",
        "    accuracy = classification(5500, 40, 0.82, value)\n",
        "    accuracy_list.append(accuracy)\n",
        "    value_list.append(value)\n",
        "\n",
        "  data = np.column_stack((accuracy_list, value_list))\n",
        "  df = pd.DataFrame(data, columns = [\"Точность\", \"value\"])\n",
        "  print(df)\n",
        "\n",
        "#classification(5500, 40, 0.82, 1000)\n",
        "#test()"
      ],
      "metadata": {
        "id": "2X52GWJVCyfp"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Опорные вектора"
      ],
      "metadata": {
        "id": "4z-BgXUOFE4k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import pickle\n",
        "\n",
        "from pandas.io.formats.style_render import DataFrame\n",
        "\n",
        "import scipy.sparse as sp\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def get_dataset(path: str) -> DataFrame:\n",
        "  return pd.read_pickle(path + \".pkl\")\n",
        "\n",
        "dataset = get_dataset(\"cleaned_dataset\")\n",
        "reviews = dataset[\"review\"]\n",
        "\n",
        "newComments = []\n",
        "for comment in reviews.tolist():\n",
        "  newComments.append(str(comment))\n",
        "\n",
        "def classification(max_feautures, min_df, max_df, kernel, degree):\n",
        "  vectorizer = TfidfVectorizer(max_features=max_feautures, min_df = min_df, max_df = max_df)\n",
        "  reviews_vectorized = vectorizer.fit_transform(newComments).toarray()\n",
        "\n",
        "  x_train, x_test, y_train, y_test = train_test_split(reviews_vectorized, dataset[\"label\"], test_size=0.3, shuffle = True)\n",
        "\n",
        "  classifier = SVC(kernel = kernel, degree = degree)\n",
        "\n",
        "  classifier.fit(x_train, y_train) \n",
        "  y_pred = classifier.predict(x_test)\n",
        "\n",
        "  print(\"Отчёт по классификации: \")\n",
        "  print(classification_report(y_test, y_pred))\n",
        "  print(\"Точность: \" + str(accuracy_score(y_test, y_pred)))\n",
        "\n",
        "  return accuracy_score(y_test, y_pred);\n",
        "\n",
        "def test():\n",
        "  accuracy_list = []\n",
        "  value_list = []\n",
        "\n",
        "  for value in range(1000, 11000, 500):\n",
        "    accuracy = classification(5500, 40, 0.82, \"poly\", 2)\n",
        "    accuracy_list.append(accuracy)\n",
        "    value_list.append(value)\n",
        "\n",
        "  data = np.column_stack((accuracy_list, value_list))\n",
        "  df = pd.DataFrame(data, columns = [\"Точность\", \"min_df\"])\n",
        "  print(df)\n",
        "\n",
        "def get(max_feautures, min_df, max_df, kernel, degree, comment):\n",
        "  reviews_string[0] = comment\n",
        "  vectorizer = TfidfVectorizer(max_features=max_feautures, min_df = min_df, max_df = max_df)\n",
        "  reviews_vectorized = vectorizer.fit_transform(reviews_string).toarray()\n",
        "\n",
        "  x_train, x_test, y_train, y_test = train_test_split(reviews_vectorized, dataset[\"label\"], test_size=0.3, shuffle = True)\n",
        "\n",
        "  classifier = SVC(kernel = kernel, degree = degree)\n",
        "  classifier.fit(x_train, y_train) \n",
        "\n",
        "  predicted = classifier.predict(x_test)\n",
        "\n",
        "\n",
        "#classification(5500, 40, 0.82, \"poly\", 2)\n",
        "#test()\n",
        "#get(5500, 40, 0.82, \"poly\", 2, comment)"
      ],
      "metadata": {
        "id": "ADQDM5GLFDdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Соседи"
      ],
      "metadata": {
        "id": "EPYjfG-fFZbP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "from pandas.io.formats.style_render import DataFrame\n",
        "\n",
        "import scipy\n",
        "from scipy import _lib\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, average_precision_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "def get_dataset(path: str) -> DataFrame:\n",
        "  return pd.read_pickle(path + \".pkl\")\n",
        "\n",
        "dataset = get_dataset(\"cleaned_dataset\")\n",
        "reviews = dataset[\"review\"]\n",
        "\n",
        "newComments = []\n",
        "for comment in reviews.tolist():\n",
        "  newComments.append(str(comment))\n",
        "\n",
        "def classification(max_feautures, min_df, max_df, n_neighbors):\n",
        "  vectorizer = TfidfVectorizer(max_features=max_feautures, min_df = min_df, max_df = max_df)\n",
        "  reviews_vectorized = vectorizer.fit_transform(newComments).toarray()\n",
        "\n",
        "  x_train, x_test, y_train, y_test = train_test_split(reviews_vectorized, dataset[\"label\"], test_size=0.3, shuffle = True)\n",
        "\n",
        "  classifier = KNeighborsClassifier(n_neighbors = n_neighbors)\n",
        "\n",
        "  classifier.fit(x_train, y_train) \n",
        "  y_pred = classifier.predict(x_test)\n",
        "\n",
        "  print(\"Отчёт по классификации: \")\n",
        "  print(classification_report(y_test, y_pred))\n",
        "  print(\"Точность: \" + str(accuracy_score(y_test, y_pred)))\n",
        "\n",
        "  return accuracy_score(y_test, y_pred);\n",
        "\n",
        "def test():\n",
        "  accuracy_list = []\n",
        "  value_list = []\n",
        "\n",
        "  for value in range(1000, 11000, 500):\n",
        "    accuracy = classification(value, 40, 0.82, 83)\n",
        "    accuracy_list.append(accuracy)\n",
        "    value_list.append(value)\n",
        "\n",
        "  data = np.column_stack((accuracy_list, value_list))\n",
        "  df = pd.DataFrame(data, columns = [\"Точность\", \"max_feautures\"])\n",
        "  print(df)\n",
        "\n",
        "def test_n():\n",
        "  vectorizer = TfidfVectorizer(max_features=5500, min_df = 40, max_df = 0.82)\n",
        "  reviews_vectorized = vectorizer.fit_transform(reviews_string).toarray()\n",
        "\n",
        "  x_train, x_test, y_train, y_test = train_test_split(reviews_vectorized, dataset[\"label\"], test_size=0.3, shuffle = True)\n",
        "\n",
        "  error_rates = []\n",
        "  for i in np.arange(1, 101):\n",
        "    new_model = KNeighborsClassifier(n_neighbors = i)\n",
        "    new_model.fit(x_train, y_train)\n",
        "    new_predictions = new_model.predict(x_test)\n",
        "    error_rates.append(np.mean(new_predictions != y_test))\n",
        "  plt.plot(error_rates)\n",
        "\n",
        "#classification(5500, 40, 0.82, 82)\n",
        "#test_n()\n",
        "#test()"
      ],
      "metadata": {
        "id": "_Q6YOAFUFgX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tensor MLP (multilayer perceptron)"
      ],
      "metadata": {
        "id": "rRUS0sePF9TB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Activation\n",
        "from keras.layers import Dropout\n",
        "\n",
        "import keras\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from pandas.io.formats.style_render import DataFrame\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "def get_dataset(path: str) -> DataFrame:\n",
        "  return pd.read_pickle(path + \".pkl\")\n",
        "\n",
        "dataset = get_dataset(\"cleaned_dataset\")\n",
        "reviews = dataset[\"review\"]\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(reviews.tolist())\n",
        "textSequences = tokenizer.texts_to_sequences(reviews.tolist())\n",
        "\n",
        "labels = dataset[\"label\"]\n",
        "labels[labels == \"bad\"] = 0\n",
        "labels[labels == \"neutral\"] = 1\n",
        "labels[labels == \"good\"] = 2\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(textSequences, labels, test_size=0.3, shuffle = True)\n",
        "\n",
        "num_words = 10000\n",
        "\n",
        "print(u'Преобразуем описания заявок в векторы чисел...')\n",
        "tokenizer = Tokenizer(num_words=num_words)\n",
        "x_train = tokenizer.sequences_to_matrix(x_train, mode='binary')\n",
        "x_test = tokenizer.sequences_to_matrix(x_test, mode='binary')\n",
        "print('Размерность X_train:', x_train.shape)\n",
        "print('Размерность X_test:', x_test.shape)\n",
        "\n",
        "print(u'Преобразуем категории в матрицу двоичных чисел '\n",
        "      u'(для использования categorical_crossentropy)')\n",
        "y_train = keras.utils.to_categorical(y_train, 3)\n",
        "y_test = keras.utils.to_categorical(y_test, 3)\n",
        "print('y_train shape:', y_train.shape)\n",
        "print('y_test shape:', y_test.shape)\n",
        "\n",
        "epochs = 10\n",
        "\n",
        "print(u'Собираем модель...')\n",
        "model = Sequential()\n",
        "model.add(Dense(512, input_shape=(num_words,)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(3))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=32,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,)\n",
        "\n",
        "score = model.evaluate(x_test, y_test,\n",
        "                       batch_size=32, verbose=1)\n",
        "print()\n",
        "print(u'Оценка теста: {}'.format(score[0]))\n",
        "print(u'Оценка точности модели: {}'.format(score[1]))"
      ],
      "metadata": {
        "id": "yaO8WhqZGEJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tensor LSTM (long short-term memory)"
      ],
      "metadata": {
        "id": "RIdnJsYTblAe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM\n",
        "\n",
        "import keras\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from pandas.io.formats.style_render import DataFrame\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "def get_dataset(path: str) -> DataFrame:\n",
        "  return pd.read_pickle(path + \".pkl\")\n",
        "\n",
        "dataset = get_dataset(\"cleaned_dataset\")\n",
        "reviews = dataset[\"review\"]\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(reviews.tolist())\n",
        "textSequences = tokenizer.texts_to_sequences(reviews.tolist())\n",
        "\n",
        "labels = dataset[\"label\"]\n",
        "labels[labels == \"bad\"] = 0\n",
        "labels[labels == \"neutral\"] = 1\n",
        "labels[labels == \"good\"] = 2\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(textSequences, labels, test_size=0.3, shuffle = True)\n",
        "\n",
        "num_words = 1000\n",
        "\n",
        "print(u'Преобразуем описания заявок в векторы чисел...')\n",
        "tokenizer = Tokenizer(num_words=num_words)\n",
        "x_train = tokenizer.sequences_to_matrix(x_train, mode='binary')\n",
        "x_test = tokenizer.sequences_to_matrix(x_test, mode='binary')\n",
        "print('Размерность X_train:', x_train.shape)\n",
        "print('Размерность X_test:', x_test.shape)\n",
        "\n",
        "print(u'Преобразуем категории в матрицу двоичных чисел '\n",
        "      u'(для использования categorical_crossentropy)')\n",
        "y_train = keras.utils.to_categorical(y_train, 3)\n",
        "y_test = keras.utils.to_categorical(y_test, 3)\n",
        "print('y_train shape:', y_train.shape)\n",
        "print('y_test shape:', y_test.shape)\n",
        "\n",
        "max_words = 0\n",
        "for desc in reviews.tolist():\n",
        "    words = len(desc)\n",
        "    if words > max_words:\n",
        "        max_words = words\n",
        "print('Максимальное количество слов в самом длинном описании заявки: {} слов'.format(max_words))\n",
        "\n",
        "maxSequenceLength = max_words\n",
        "\n",
        "print(u'Собираем модель...')\n",
        "model = Sequential()\n",
        "model.add(Embedding(1000, maxSequenceLength))\n",
        "model.add(LSTM(32, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(3, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "print (model.summary())\n",
        "\n",
        "batch_size = 32\n",
        "epochs = 3\n",
        "\n",
        "print(u'Тренируем модель...')\n",
        "history = model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          validation_data=(x_test, y_test))\n",
        "\n",
        "score = model.evaluate(x_test, y_test,\n",
        "                       batch_size=batch_size, verbose=1)\n",
        "print()\n",
        "print(u'Оценка теста: {}'.format(score[0]))\n",
        "print(u'Оценка точности модели: {}'.format(score[1]))"
      ],
      "metadata": {
        "id": "FXdMYdx_bkrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Векторизация датасета и обучение модели"
      ],
      "metadata": {
        "id": "fdRXqelcjXpF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = get_dataset(\"cleaned_dataset\")\n",
        "reviews = dataset[\"review\"]\n",
        "\n",
        "reviewsToVectorizer = []\n",
        "for review in reviews.tolist():\n",
        "  reviewsToVectorizer.append(str(review))\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_features=5500, min_df = 40, max_df = 0.82)\n",
        "reviews_vectorized = vectorizer.fit_transform(reviewsToVectorizer).toarray()\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(reviews_vectorized, dataset[\"label\"], test_size=0.3)\n",
        "\n",
        "classifier = SVC(kernel = \"poly\", degree = 2)\n",
        "\n",
        "classifier.fit(x_train, y_train) "
      ],
      "metadata": {
        "id": "CacBJc-QSsih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Пользовательский ввод"
      ],
      "metadata": {
        "id": "-GUHuK3kSvIB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_review = input(\"Введите комментарий для оценки: \")\n",
        "input_cleaned_review = get_cleaned_reviews([input_review])\n",
        "\n",
        "input_vec = vectorizer.transform([str(input_cleaned_review)]).toarray()\n",
        "\n",
        "predicted = classifier.predict(input_vec)\n",
        "\n",
        "if (predicted[0] == \"bad\"):\n",
        "  print(\"Эмоциональная оценка коментария - негативный.\")\n",
        "if (predicted[0] == \"good\"):\n",
        "  print(\"Эмоциональная оценка коментария - позитивный.\")\n",
        "if (predicted[0] == \"neutral\"):\n",
        "  print(\"Эмоциональная оценка коментария - нейтральный.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BF1gkJvFiuQ5",
        "outputId": "300ee7a5-c73c-4d41-810b-21b81ce4b2bb"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Введите комментарий для оценки: Плохой\n",
            "Эмоциональная оценка коментария - негативный.\n"
          ]
        }
      ]
    }
  ]
}